---
title: "Assignment 10 â€“ Sentiment Analysis"
author: "Matthew Lucich"
output:
  html_document: default
  pdf_document: default
editor_options: 
  chunk_output_type: console
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(eval = TRUE, results = TRUE, fig.show = "show", message = FALSE)
```

```{r load-packages}
library(tidyverse)
library(ggplot2)
library(tidytext)
library(textdata)
library(janeaustenr)
library(gutenbergr)
library(SentimentAnalysis)
```

## Overview

The objective was to re-create the code supplied in chapter 2 of *Text Mining with R*, then to extend the exercise with a new corpus and lexicon. Recreating the analysis in chapter 2 came with no issues, thanks to the thorough steps provided by the textbook. Extending the analysis brought up some issues with dealing with the text formatting, particularly finding the right regex to partition chapters in the new corpus. Finding an additional lexicon to utilize in the exercise was straightforward as the SentimentAnalysis R package came equipped with multiple dictionaries, of which we used the Harvard-IV dictionary.


### Tidy text: Jane Austen books

```{r}

tidy_books <- austen_books() %>%
  group_by(book) %>%
  mutate(
    linenumber = row_number(),
    chapter = cumsum(str_detect(text, 
                                regex("^chapter [\\divxlc]", 
                                      ignore_case = TRUE)))) %>%
  ungroup() %>%
  unnest_tokens(word, text)

head(tidy_books)

```


### Tidy text: Baruch Spinoza books

```{r}

# Download Ethics by Spinoza from gutenbergr
ethics_raw <- gutenberg_download(3800)
ethics <- as_tibble(ethics_raw)
ethics <- ethics %>% add_column(book = "Ethics")

# Download Theologico-Political by Spinoza from gutenbergr
theologico_prt1_raw <- gutenberg_download(989)
theologico_prt1 <- as_tibble(theologico_prt1_raw)
theologico_prt1 <- theologico_prt1 %>% add_column(book = "Theologico-Political Part 1")

# Concat Spinoza books into one dataframe
spinoza_books_raw <- bind_rows(ethics, theologico_prt1)

# Tokenize the text so that each word is its own row
spinoza_books <- spinoza_books_raw %>%
  group_by(book) %>%
  mutate(
    linenumber = row_number(),
    chapter = cumsum(str_detect(text, 
                                regex("PART|Chapter|CHAPTER  [\\dIVXLC]")))) %>%
  ungroup() %>%
  unnest_tokens(word, text)

tail(spinoza_books)

```



## Filter for words by sentiment using nrc: Emma, by Jane Austen

```{r}

nrc_joy <- get_sentiments("nrc") %>% 
  filter(sentiment == "joy")

tidy_books %>%
  filter(book == "Emma") %>%
  inner_join(nrc_joy) %>%
  count(word, sort = TRUE)

```


## Filter for words by sentiment using Harvard-IV: Ethics, by Baruch Spinoza

```{r}

# Filter for words with positive sentiment
# Note: DictionaryGI is Harvard-IV dictionary from SentimentAnalysis
gi_positive <- as_tibble(DictionaryGI$positive)
gi_positive <- gi_positive %>% rename(word = value)

# Filter for words with negative sentiment (DictionaryGI is from SentimentAnalysis)
gi_negative <- as_tibble(DictionaryGI$negative)
gi_negative <- gi_negative %>% rename(word = value)

# Inner join words from Ethics and positive words from Harvard-IV lexicon
spinoza_positive <- spinoza_books %>%
                        filter(book == "Ethics") %>%
                        inner_join(gi_positive) %>%
                        count(word, sort = TRUE)

head(spinoza_positive)

```


### Count positive vs negative words in 80 line sections: Jane Austen

```{r}

jane_austen_sentiment <- tidy_books %>%
  inner_join(get_sentiments("bing")) %>%
  count(book, index = linenumber %/% 80, sentiment) %>%
  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>%
  mutate(sentiment = positive - negative)

head(jane_austen_sentiment)

```


### Count positive vs negative words in 80 line sections: Baruch Spinoza

```{r}

# Add column indicating sentiment (needed in long format)
gi_positive_df <- gi_positive %>% add_column(sentiment = "positive")
gi_negative_df <- gi_negative %>% add_column(sentiment = "negative")

# Concat positive and negative words into one dataframe
gi_dict <- bind_rows(gi_positive_df, gi_negative_df)

# Evaluate sentiment 80 lines at a time
spinoza_sentiment <- spinoza_books %>%
  inner_join(gi_dict) %>%
  count(book, index = linenumber %/% 80, sentiment) %>%
  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>%
  mutate(sentiment = positive - negative)

head(spinoza_sentiment)

```


### Plot sentiment by book: Jane Austen

```{r}

ggplot(jane_austen_sentiment, aes(index, sentiment, fill = book)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~book, ncol = 2, scales = "free_x")

```



### Plot sentiment by book: Baruch Spinoza

```{r}

# View net sentiment over the course of the book
ggplot(spinoza_sentiment, aes(index, sentiment, fill = book)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~book, ncol = 2, scales = "free_x")

```


### Comparing sentiment classification across lexicons: Pride & Prejudice

```{r}

pride_prejudice <- tidy_books %>% 
  filter(book == "Pride & Prejudice")

afinn <- pride_prejudice %>% 
  inner_join(get_sentiments("afinn")) %>% 
  group_by(index = linenumber %/% 80) %>% 
  summarise(sentiment = sum(value)) %>% 
  mutate(method = "AFINN")

bing_and_nrc <- bind_rows(
  pride_prejudice %>% 
    inner_join(get_sentiments("bing")) %>%
    mutate(method = "Bing et al."),
  pride_prejudice %>% 
    inner_join(gi_dict %>% 
                 filter(sentiment %in% c("positive", 
                                         "negative"))
    ) %>%
    mutate(method = "NRC")) %>%
  count(method, index = linenumber %/% 80, sentiment) %>%
  pivot_wider(names_from = sentiment,
              values_from = n,
              values_fill = 0) %>% 
  mutate(sentiment = positive - negative)

bind_rows(afinn, 
          bing_and_nrc) %>%
  ggplot(aes(index, sentiment, fill = method)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~method, ncol = 1, scales = "free_y")

```



### Comparing sentiment classification across lexicons: Ethics

```{r}

ethics_book <- spinoza_books %>%
                        filter(book == "Ethics")

afinn <- ethics_book %>% 
  inner_join(get_sentiments("afinn")) %>% 
  group_by(index = linenumber %/% 80) %>% 
  summarise(sentiment = sum(value)) %>% 
  mutate(method = "AFINN")

bing_and_nrc <- bind_rows(
  ethics_book %>% 
    inner_join(get_sentiments("bing")) %>%
    mutate(method = "Bing et al."),
  ethics_book %>% 
    inner_join(gi_dict %>% 
                 filter(sentiment %in% c("positive", 
                                         "negative"))
    ) %>%
    mutate(method = "Harvarad-IV")) %>%
  count(method, index = linenumber %/% 80, sentiment) %>%
  pivot_wider(names_from = sentiment,
              values_from = n,
              values_fill = 0) %>% 
  mutate(sentiment = positive - negative)

bind_rows(afinn, 
          bing_and_nrc) %>%
  ggplot(aes(index, sentiment, fill = method)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~method, ncol = 1, scales = "free_y")

```



### Word counts by sentiment: Pride & Prejudice

```{r}

bing_word_counts <- tidy_books %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup()

bing_word_counts %>%
  group_by(sentiment) %>%
  slice_max(n, n = 10) %>% 
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(x = "Contribution to sentiment",
       y = NULL)

```



### Word counts by sentiment: Ethics

```{r}

# Label the sentiment of each word
gi_word_counts <- spinoza_books %>%
  inner_join(gi_dict) %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup()

# Plot top negative and positive words by count 
gi_word_counts %>%
  group_by(sentiment) %>%
  slice_max(n, n = 10) %>% 
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(x = "Contribution to sentiment",
       y = NULL)

```


### Add custom stop words: Jane Austen

```{r}

custom_stop_words_ja <- bind_rows(tibble(word = c("miss"),  
                                      lexicon = c("custom")), 
                               stop_words)

custom_stop_words_ja

```


### Add custom stop words: Baruch Spinoza

```{r}

custom_stop_words_sp <- bind_rows(tibble(word = c("mind", "order"),  
                                      lexicon = c("custom", "custom")), 
                               stop_words)

custom_stop_words_sp

```


### Sentence and chapter tokenization: Pride & Prejudice

```{r}

p_and_p_sentences <- tibble(text = prideprejudice) %>% 
  unnest_tokens(sentence, text, token = "sentences")

austen_chapters <- austen_books() %>%
  group_by(book) %>%
  unnest_tokens(chapter, text, token = "regex", 
                pattern = "Chapter|CHAPTER [\\dIVXLC]") %>%
  ungroup()

austen_chapters %>% 
  group_by(book) %>% 
  summarise(chapters = n())

```


### Sentence and chapter tokenization: Ethics

```{r}

ethics_text <- spinoza_books_raw %>%
                        filter(book == "Ethics")

# Tokenize chapters with regex 
spinoza_chapters <- spinoza_books_raw %>%
  group_by(book) %>%
  unnest_tokens(chapter, text, token = "regex", 
                pattern = "PART|Chapter|CHAPTER  [\\dIVXLC]") %>%
                ungroup()

# Confirm chapters were partitioned correctly
spinoza_chapters %>% 
  group_by(book) %>% 
  summarise(chapters = n())

```


### Negative word ratio: Jane Austen

```{r}

bingnegative <- get_sentiments("bing") %>% 
  filter(sentiment == "negative")

wordcounts <- tidy_books %>%
  group_by(book, chapter) %>%
  summarize(words = n())

tidy_books %>%
  semi_join(bingnegative) %>%
  group_by(book, chapter) %>%
  summarize(negativewords = n()) %>%
  left_join(wordcounts, by = c("book", "chapter")) %>%
  mutate(ratio = negativewords/words) %>%
  filter(chapter != 0) %>%
  slice_max(ratio, n = 1) %>% 
  ungroup()

```



### Positive word ratio: Baruch Spinoza

```{r}

bingpositive <- gi_dict %>% 
  filter(sentiment == "positive")

# Word count of each chapter
wordcounts_sp <- spinoza_books %>%
  group_by(book, chapter) %>%
  summarize(words = n())

# Highest positive word ratio of each book
spinoza_books %>%
  semi_join(bingpositive) %>%
  group_by(book, chapter) %>%
  summarize(positivewords = n()) %>%
  left_join(wordcounts_sp, by = c("book", "chapter")) %>%
  mutate(ratio = positivewords/words) %>%
  slice_max(ratio, n = 1) %>% 
  ungroup()

```


## Conclusion

Again, the meticulous documentation by *Text Mining with R* made re-creating the primary code functions seamless. There are two main takeaways from extending the sentiment analysis. One, this task once again proves that the majority of a data scientists time will be formatting the data. Two, these analyses will need customization. For example, many of the positive and negative sentiment words are questionable in the context of this text. When looking to build sentiment analysis into a production application, thorough review of the dictionaries being used will be required. Additionally, the data scientist should consider using a domain-specific lexicon when possible. While all the lexicons used in this analysis had their weak points, AFINN seems to be the strongest as it offers degrees of positive and negative sentiment.


....



## References

Julia Silge and David Robinson (2017) [Chapter 2, Text Mining with R](https://www.tidytextmining.com/sentiment.html)

David Robinson (2020). gutenbergr: Search and download public domain texts from Project Gutenberg. R package version 	0.2.0.

Stefan Feuerriegel, Nicolas Proellochs (2021). SentimentAnalysis: a powerful toolchain facilitating the sentiment analysis of textual contents in R. R package version 1.3-4.



* * *

<a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-sa/4.0/88x31.png" /></a><br />This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>.

